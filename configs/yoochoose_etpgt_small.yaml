# ETP-GT Small Configuration
# TODO: Populate in Phase 5

model_type: etpgt

# Model architecture
model:
  hidden_dim: 256
  num_layers: 3
  num_heads: 4
  dropout: 0.2
  drop_path: 0.1
  activation: swiglu
  use_layer_scale: false
  
  # Positional encodings
  lappe_k: 16
  temporal_buckets: [60, 300, 1800, 7200, 86400, 604800, inf]  # seconds
  path_buckets: [1, 2, 3]  # 3 means 3+
  
  # Readout
  use_cls_token: true
  use_gated_pooling: true

# Sampling
sampling:
  fanout: [16, 12, 8]
  last_n: 10
  max_edges_per_batch: 10000

# Training
training:
  batch_size: 256
  epochs: 50
  lr: 0.001
  weight_decay: 0.0001
  warmup_steps_ratio: 0.05
  gradient_clip: 1.0
  early_stop_patience: 5
  early_stop_metric: ndcg@20
  use_amp: true  # Mixed precision

# Loss
loss:
  listwise_weight: 0.7
  contrastive_weight: 0.3
  candidates_k: 200

# Data
data:
  train_path: gs://<bucket>/data/processed/train.parquet
  val_path: gs://<bucket>/data/processed/val.parquet
  test_path: gs://<bucket>/data/processed/test.parquet
  graph_path: gs://<bucket>/data/interim/graph_edges.parquet

# Evaluation
eval:
  k_values: [10, 20]
  stratify_by:
    - session_length
    - last_gap_dt
    - cold_items

# Logging
logging:
  wandb_project: etp-gt
  wandb_entity: null
  log_interval: 100
  save_dir: gs://<bucket>/artifacts/etpgt

# Reproducibility
seed: 42

