# Vertex AI CustomJob config: GPU smoke test on L4

# Use pipeline service account for proper API scopes/permissions
serviceAccount: sa-pipeline@plotpointe.iam.gserviceaccount.com

workerPoolSpecs:
- machineSpec:
    machineType: g2-standard-8
    acceleratorType: NVIDIA_L4
    acceleratorCount: 1
  replicaCount: 1
  containerSpec:
    imageUri: us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-cu124.2-4.py310
    command: ["bash", "-lc"]
    args:
      - |
        set -e
        echo "Python: $(python -V 2>&1)"
        echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
        nvidia-smi || true
        python -m pip install --quiet --no-cache-dir google-cloud-aiplatform
        python - <<'PY'
        import time
        from google.cloud import aiplatform
        import torch

        PROJECT="plotpointe"
        LOCATION="us-central1"
        EXPERIMENT="recsys-dev"
        run_name=f"gpu-smoke-l4-{int(time.time())}"

        print("torch:", torch.__version__, "cuda available:", torch.cuda.is_available())
        if not torch.cuda.is_available():
            raise SystemExit("CUDA not available")
        device = torch.device("cuda:0")
        name = torch.cuda.get_device_name(0)
        start = time.perf_counter()
        a = torch.randn(2048, 2048, device=device)
        b = torch.randn(2048, 2048, device=device)
        c = torch.mm(a, b)
        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        print(f"DEVICE={name}")
        print(f"ELAPSED_SEC={elapsed:.4f}")

        aiplatform.init(project=PROJECT, location=LOCATION, experiment=EXPERIMENT)
        with aiplatform.start_run(run=run_name):
            aiplatform.log_params({"profile":"gpu-l4", "device": name})
            aiplatform.log_metrics({"cuda_ok": 1.0, "elapsed_sec": elapsed})
        PY

